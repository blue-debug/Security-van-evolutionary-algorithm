{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, id = 4860690891018271):\n",
    "        self.id = id\n",
    "        self.mid = id\n",
    "        \n",
    "        self.cookie = 'WEIBOCN_FROM=1110006030; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfqKKRqnDvCnrW-FINRVD2deutyyUpVAz_qetv-sFRd5J4.; SUB=_2A25O0rvLDeRhGeBP71YU8y7KzTuIHXVqPMWDrDV6PUJbktAKLVbCkW1NRUYLcSM1wE7gRu66wcI6nrX-vEo6cBuK; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SSOLoginState=1675021211; ALF=1677613211; _T_WM=15194666543; MLOGIN=1; M_WEIBOCN_PARAMS=oid%3D4860690891018271%26luicode%3D20000061%26lfid%3D4860690891018271%26uicode%3D20000061%26fid%3D4860690891018271; XSRF-TOKEN=926c8e; mweibo_short_token=a1ade2f776'\n",
    "\n",
    "        self.url = f'https://m.weibo.cn/statuses/extend?id={self.id}'\n",
    "\n",
    "        r = requests.get(self.url, timeout = 5)\n",
    "        string = r.json()['data']['longTextContent']\n",
    "        # df = pd.DataFrame(columns = [\"index\", \"text\"])\n",
    "        # df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"name\": [''], \n",
    "                \"粉丝数\": [''], \n",
    "                \"like_count\": [''], \n",
    "                \"source\": [''], \n",
    "                \"text\": [string]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        df.to_csv(\"spider.csv\", index = False)\n",
    "    \n",
    "\n",
    "    def sub_comment(self, comment_id):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id + '?cid=' + comment_id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        # print(headers)\n",
    "\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "        var_index, var_text = [], []\n",
    "        var_like_count = []\n",
    "        var_name = []\n",
    "        var_fans = []\n",
    "        var_source = []\n",
    "\n",
    "        while times < max:\n",
    "\n",
    "            url = f'https://m.weibo.cn/comments/hotFlowChild?cid={comment_id}&max_id={max_id}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = cards['data']\n",
    "\n",
    "                    for i in cards:\n",
    "                        # print(i['text'])\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "                        var_source.append(i['source'])\n",
    "                    \n",
    "                    max_id = r.json()['max_id']\n",
    "                    id_type = r.json()['max_id_type']\n",
    "                    max = r.json()['max']\n",
    "\n",
    "                    # time.sleep(1)\n",
    "                    times += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"return data is null\")\n",
    "                    break\n",
    "        \n",
    "        return [var_name, var_fans, var_text, var_like_count, var_source]\n",
    "    \n",
    "\n",
    "    def comment(self):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        # print(\"main\")\n",
    "        # print(headers)\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "\n",
    "        while times < max:\n",
    "            print(f'当前次数{times}为， 总共{max}次')\n",
    "            url = f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id={max_id}&max_id_type={id_type}' if max_id >= 1 else f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            \n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = r.json()['data']['data']\n",
    "                    var_text = []\n",
    "                    var_like_count = []\n",
    "                    var_name = []\n",
    "                    var_fans = []\n",
    "                    var_source = []\n",
    "\n",
    "                    time.sleep(random.uniform(0, 2))\n",
    "                    times += 1 \n",
    "\n",
    "                    # try:\n",
    "                    for i in cards:\n",
    "                        # print(i['id'], i['text'])\n",
    "\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "                        var_source.append(i['source'])\n",
    "\n",
    "                        return_var = self.sub_comment(i['id'])\n",
    "                        var_name += return_var[0]\n",
    "                        var_fans.extend(return_var[1])\n",
    "                        var_text += return_var[2]\n",
    "                        var_like_count.extend(return_var[3])\n",
    "                        var_source += return_var[4]\n",
    "\n",
    "                    df = pd.DataFrame(\n",
    "                        {\n",
    "                            \"name\": var_name, \n",
    "                            \"粉丝数\": var_fans,  \n",
    "                            \"like_count\": var_like_count, \n",
    "                            \"source\": var_source, \n",
    "                            \"text\": var_text\n",
    "                        }\n",
    "                    )\n",
    "                    # df = df.append({\"index\": cards['id'], \"text\": cards['text']}, ignore_index = True)\n",
    "                    \n",
    "                    df.to_csv(\"spider.csv\", index = False, mode = 'a', header = False)\n",
    "\n",
    "                    if times == 1: max = r.json()['data']['max']\n",
    "\n",
    "                    max_id = r.json()['data']['max_id']\n",
    "                    id_type = r.json()['data']['max_id_type']\n",
    "                \n",
    "                else:\n",
    "                    print(\"data is null\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(\"max not found\")\n",
    "            #     print(e)\n",
    "            #     continue\n",
    "    \n",
    "    def trans_time(self, v_str):\n",
    "        \"\"\"转换GMT时间为标准格式\"\"\"\n",
    "        GMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "        timeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "        ret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return ret_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        spider = Crawler('4860690891018271').comment()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user. Exiting...\")\n",
    "        del spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京大学 3237705130\n",
      "清华大学 1676317545\n",
      "北京师范大学 1875088617\n",
      "北京外国语大学 1873032605\n",
      "北京交通大学 1865345137\n",
      "北京邮电大学 1844283341\n",
      "中国政法大学 1853914071\n",
      "中央财经大学 3113890133\n",
      "中央美术学院 1998766813\n",
      "对外经济贸易大学 1980761473\n",
      "南开大学 1851774145\n",
      "天津大学 1871729063\n",
      "大连理工大学 3082822153\n",
      "东北大学 1891849065\n",
      "吉林大学 2271848313\n",
      "东北师范大学 1924502695\n",
      "复旦大学 1729332983\n",
      "上海交通大学 1739746697\n",
      "同济大学 1865154537\n",
      "华东理工大学 2028734565\n",
      "东华大学 1880834235\n",
      "华东师范大学 1688511087\n",
      "上海外国语大学 1887685537\n",
      "东南大学 1703010470\n",
      "河海大学 1853565117\n",
      "江南大学 1854563221\n",
      "南京农业大学 1905782710\n",
      "浙江大学 1851755225\n",
      "厦门大学 1879192935\n",
      "山东大学 1880883723\n",
      "中国海洋大学 1825107551\n",
      "中国石油大学（华东) 1865978564\n",
      "武汉大学 1666177401\n",
      "华中科技大学 1663414103\n",
      "武汉理工大学 1899783701\n",
      "华中农业大学 1690023261\n",
      "中南财经政法大学 1216903164\n",
      "湖南大学 1851726313\n",
      "中南大学 1871765890\n",
      "中山大学 1892723783\n",
      "重庆大学 1875333245\n",
      "西南大学 1973665271\n",
      "西南交通大学 2418433987\n",
      "西安交通大学 1862928653\n",
      "西北农林科技大学 3204433793\n",
      "陕西师范大学 1833031745\n",
      "西安电子科技大学 1854580681\n",
      "兰州大学 1878134457\n",
      "哈尔滨工业大学 1873625985\n",
      "哈尔滨工程大学 1865804041\n",
      "南京航空航天大学 1805101535\n",
      "中国科学技术大学 1823887605\n",
      "北京电影学院 1875379117\n",
      "辽宁大学 1898999173\n",
      "上海大学 3243026514\n",
      "南京林业大学 2237972353\n",
      "浙江工商大学 3317008062\n",
      "安徽大学 1878323355\n",
      "河南大学 1827578857\n",
      "武汉纺织大学 2579889714\n",
      "广西大学 1968274691\n",
      "西南政法大学 1916627235\n",
      "四川师范大学 2034194434\n",
      "贵州大学 1847300301\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "# https://m.weibo.cn/profile/1764661771\n",
    "\n",
    "def getUserId(userName, pageNum):\n",
    "    name = urllib.parse.quote('=1&q=' + userName)\n",
    "    url = f'https://m.weibo.cn/api/container/getIndex?containerid=100103type{name}&page_type=searchall'\n",
    "\n",
    "    # print('https://m.weibo.cn/api/container/getIndex?containerid=100103type%3D1%26q%3D%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6&page_type=searchall')\n",
    "    # print(url)\n",
    "\n",
    "    headers = {\n",
    "    'Cookie': '_T_WM=69514023915; MLOGIN=1; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfq6k41yT5HWkNhZ5TIvDrF5_t-RjdCZ-1J3mknscKAxjU.; SUB=_2A25O3rybDeRhGeBP71YU8y7KzTuIHXVqIMTTrDV6PUJbktANLWfxkW1NRUYLcUYi8i0Y-SvhDTT3Qd54PBXW9h3c; SSOLoginState=1675283659; ALF=1677875659; WEIBOCN_FROM=1110106030; XSRF-TOKEN=1293ca; mweibo_short_token=60cc77794f; M_WEIBOCN_PARAMS=luicode%3D10000011%26lfid%3D231051_-_followers_-_1764661771%26fid%3D231051_-_followers_-_1764661771%26uicode%3D10000011',\n",
    "    'Referer': 'https://m.weibo.cn/search?containerid=100103type' + name,\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, headers = headers, timeout = 5)\n",
    "    # print(r.json())\n",
    "    uid = r.json()['data']['cards'][0]['card_group'][0]['user']['id']\n",
    "    return uid\n",
    "\n",
    "def scoll_search():\n",
    "    pass\n",
    "\n",
    "\n",
    "uni_list = ['北京大学','清华大学','北京师范大学','北京外国语大学','北京交通大学','北京邮电大学','中国政法大学','中央财经大学','中央美术学院','对外经济贸易大学','南开大学','天津大学','大连理工大学','东北大学','吉林大学','东北师范大学','复旦大学','上海交通大学','同济大学','华东理工大学','东华大学','华东师范大学','上海外国语大学','南京大学','东南大学','河海大学','江南大学','南京农业大学','浙江大学','厦门大学','山东大学','中国海洋大学','中国石油大学（华东)','武汉大学','华中科技大学','中国地质大学（武汉)','武汉理工大学','华中农业大学','中南财经政法大学','湖南大学','中南大学','中山大学','重庆大学','西南大学','西南交通大学','西安交通大学','西北农林科技大学','陕西师范大学','西安电子科技大学','兰州大学','哈尔滨工业大学','哈尔滨工程大学','南京航空航天大学','中国科学技术大学','北京电影学院','辽宁大学','上海大学','南京林业大学','浙江工商大学','安徽大学','河南大学','武汉纺织大学','广西大学','西南政法大学','四川师范大学','贵州大学']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in uni_list:\n",
    "        try:\n",
    "            print(i, getUserId(i, \"1\"))\n",
    "        except:\n",
    "            print(i, \"not found\")\n",
    "            continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 草稿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = \"[\\u4e00-\\u9fa5]{5}\"\n",
    "\n",
    "# matches = re.findall(pattern, string)\n",
    "\n",
    "# if matches:\n",
    "#     for match in matches:\n",
    "#         print(match, end = \"\")\n",
    "# else:\n",
    "#     print(\"匹配失败\")\n",
    "\n",
    "# except:\n",
    "# \t\tcontinue\n",
    "\n",
    "# finally:\n",
    "# \ttime.sleep(random.uniform(0, 2))\n",
    "# \ttimes += 1\n",
    "\n",
    "# # 转发数\n",
    "# reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "# # 评论数\n",
    "# comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "# # 点赞数\n",
    "# attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "\n",
    "# # 请求地址\n",
    "# url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "# # 请求参数\n",
    "# params = {\n",
    "# \t\"containerid\": \"100103type=1&q={}\".format(topic),\n",
    "# \t\"page_type\": \"searchall\",\n",
    "# \t# \"page\": searchall\n",
    "# }\n",
    "# r = requests.get(url, headers = headers, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        return requests.get(url).text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'Crawling: {url}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://www.imdb.com/']).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
