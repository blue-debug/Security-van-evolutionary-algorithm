{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评论爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, id = 4860690891018271):\n",
    "        self.id = id\n",
    "        self.mid = id\n",
    "        \n",
    "        self.cookie = 'WEIBOCN_FROM=1110006030; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfqKKRqnDvCnrW-FINRVD2deutyyUpVAz_qetv-sFRd5J4.; SUB=_2A25O0rvLDeRhGeBP71YU8y7KzTuIHXVqPMWDrDV6PUJbktAKLVbCkW1NRUYLcSM1wE7gRu66wcI6nrX-vEo6cBuK; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SSOLoginState=1675021211; ALF=1677613211; _T_WM=15194666543; MLOGIN=1; M_WEIBOCN_PARAMS=oid%3D4860690891018271%26luicode%3D20000061%26lfid%3D4860690891018271%26uicode%3D20000061%26fid%3D4860690891018271; XSRF-TOKEN=926c8e; mweibo_short_token=a1ade2f776'\n",
    "\n",
    "        self.url = f'https://m.weibo.cn/statuses/extend?id={self.id}'\n",
    "\n",
    "        r = requests.get(self.url, timeout = 5)\n",
    "        string = r.json()['data']['longTextContent']\n",
    "        # df = pd.DataFrame(columns = [\"index\", \"text\"])\n",
    "        # df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"name\": [''], \n",
    "                \"粉丝数\": [''], \n",
    "                \"like_count\": [''], \n",
    "                \"source\": [''], \n",
    "                \"text\": [string]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        df.to_csv(\"spider.csv\", index = False)\n",
    "    \n",
    "\n",
    "    def sub_comment(self, comment_id):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id + '?cid=' + comment_id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        # print(headers)\n",
    "\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "        var_index, var_text = [], []\n",
    "        var_like_count = []\n",
    "        var_name = []\n",
    "        var_fans = []\n",
    "        var_source = []\n",
    "\n",
    "        while times < max:\n",
    "\n",
    "            url = f'https://m.weibo.cn/comments/hotFlowChild?cid={comment_id}&max_id={max_id}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = cards['data']\n",
    "\n",
    "                    for i in cards:\n",
    "                        # print(i['text'])\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "                        var_source.append(i['source'])\n",
    "                    \n",
    "                    max_id = r.json()['max_id']\n",
    "                    id_type = r.json()['max_id_type']\n",
    "                    max = r.json()['max']\n",
    "\n",
    "                    # time.sleep(1)\n",
    "                    times += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"return data is null\")\n",
    "                    break\n",
    "        \n",
    "        return [var_name, var_fans, var_text, var_like_count, var_source]\n",
    "    \n",
    "\n",
    "    def comment(self):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        # print(\"main\")\n",
    "        # print(headers)\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "\n",
    "        while times < max:\n",
    "            print(f'当前次数{times}为， 总共{max}次')\n",
    "            url = f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id={max_id}&max_id_type={id_type}' if max_id >= 1 else f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            \n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = r.json()['data']['data']\n",
    "                    var_text = []\n",
    "                    var_like_count = []\n",
    "                    var_name = []\n",
    "                    var_fans = []\n",
    "                    var_source = []\n",
    "\n",
    "                    time.sleep(random.uniform(0, 2))\n",
    "                    times += 1 \n",
    "\n",
    "                    # try:\n",
    "                    for i in cards:\n",
    "                        # print(i['id'], i['text'])\n",
    "\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "                        var_source.append(i['source'])\n",
    "\n",
    "                        return_var = self.sub_comment(i['id'])\n",
    "                        var_name += return_var[0]\n",
    "                        var_fans.extend(return_var[1])\n",
    "                        var_text += return_var[2]\n",
    "                        var_like_count.extend(return_var[3])\n",
    "                        var_source += return_var[4]\n",
    "\n",
    "                    df = pd.DataFrame(\n",
    "                        {\n",
    "                            \"name\": var_name, \n",
    "                            \"粉丝数\": var_fans,  \n",
    "                            \"like_count\": var_like_count, \n",
    "                            \"source\": var_source, \n",
    "                            \"text\": var_text\n",
    "                        }\n",
    "                    )\n",
    "                    # df = df.append({\"index\": cards['id'], \"text\": cards['text']}, ignore_index = True)\n",
    "                    \n",
    "                    df.to_csv(\"spider.csv\", index = False, mode = 'a', header = False)\n",
    "\n",
    "                    if times == 1: max = r.json()['data']['max']\n",
    "\n",
    "                    max_id = r.json()['data']['max_id']\n",
    "                    id_type = r.json()['data']['max_id_type']\n",
    "                \n",
    "                else:\n",
    "                    print(\"data is null\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(\"max not found\")\n",
    "            #     print(e)\n",
    "            #     continue\n",
    "    \n",
    "    def trans_time(self, v_str):\n",
    "        \"\"\"转换GMT时间为标准格式\"\"\"\n",
    "        GMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "        timeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "        ret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return ret_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        spider = Crawler('4860690891018271').comment()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user. Exiting...\")\n",
    "        del spider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户数据爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "headers = {\n",
    "    'Cookie': '_T_WM=69514023915; MLOGIN=1; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfq6k41yT5HWkNhZ5TIvDrF5_t-RjdCZ-1J3mknscKAxjU.; SUB=_2A25O3rybDeRhGeBP71YU8y7KzTuIHXVqIMTTrDV6PUJbktANLWfxkW1NRUYLcUYi8i0Y-SvhDTT3Qd54PBXW9h3c; SSOLoginState=1675283659; ALF=1677875659; WEIBOCN_FROM=1110106030; XSRF-TOKEN=1293ca; mweibo_short_token=60cc77794f; M_WEIBOCN_PARAMS=luicode%3D10000011%26lfid%3D231051_-_followers_-_1764661771%26fid%3D231051_-_followers_-_1764661771%26uicode%3D10000011',\n",
    "    'Referer': 'https://m.weibo.cn/search?containerid=100103type',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "\n",
    "\n",
    "def getUserId(userName):\n",
    "    name = urllib.parse.quote('=1&q=' + userName)\n",
    "    \n",
    "    headers = {\n",
    "    'Cookie': '_T_WM=69514023915; MLOGIN=1; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfq6k41yT5HWkNhZ5TIvDrF5_t-RjdCZ-1J3mknscKAxjU.; SUB=_2A25O3rybDeRhGeBP71YU8y7KzTuIHXVqIMTTrDV6PUJbktANLWfxkW1NRUYLcUYi8i0Y-SvhDTT3Qd54PBXW9h3c; SSOLoginState=1675283659; ALF=1677875659; WEIBOCN_FROM=1110106030; XSRF-TOKEN=1293ca; mweibo_short_token=60cc77794f; M_WEIBOCN_PARAMS=luicode%3D10000011%26lfid%3D231051_-_followers_-_1764661771%26fid%3D231051_-_followers_-_1764661771%26uicode%3D10000011',\n",
    "    'Referer': 'https://m.weibo.cn/search?containerid=100103type' + name,\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "    # https://m.weibo.cn/profile/1764661771\n",
    "\n",
    "    # headers['Referer'] = 'https://m.weibo.cn/search?containerid=100103type' + name,\n",
    "    \n",
    "    # print(type(headers['Referer']))\n",
    "\n",
    "\n",
    "    url = f'https://m.weibo.cn/api/container/getIndex?containerid=100103type{name}&page_type=searchall'\n",
    "    r = requests.get(url, headers = headers, timeout = 10)\n",
    "\n",
    "    # print(r.json())\n",
    "    try:\n",
    "        uid = r.json()['data']['cards'][0]['card_group'][0]['user']['id']\n",
    "        follow = r.json()['data']['cards'][0]['card_group'][0]['user']['follow_count']\n",
    "        fans = r.json()['data']['cards'][0]['card_group'][0]['user']['followers_count']\n",
    "        posts = r.json()['data']['cards'][0]['card_group'][0]['user']['statuses_count']\n",
    "    except:\n",
    "        del r\n",
    "        return ['', userName, '', '', '']\n",
    "    # print(userName, follow, fans, posts)\n",
    "\n",
    "    return [str(uid), userName, follow, fans, posts]\n",
    "\n",
    "\n",
    "def scoll_search(uid):\n",
    "    if uid == '7579562933': return 0\n",
    "\n",
    "    # https://m.weibo.cn/p/index?containerid=231051_-_followers_-_1865345137\n",
    "\n",
    "    headers['Referer'] = f'https://m.weibo.cn/p/index?containerid=231051_-_followers_-_{uid}'\n",
    "\n",
    "    url = f'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_{uid}'\n",
    "    # print(url)\n",
    "    # print(headers)\n",
    "    r = requests.get(url, headers = headers, timeout = 5)\n",
    "    # print(r.json())\n",
    "\n",
    "    # print(r.json()['data']['cards'][-1])\n",
    "    try:\n",
    "        name_list = []\n",
    "\n",
    "        fan_list = r.json()['data']['cards'][-1]['card_group']\n",
    "        for i in fan_list:\n",
    "            id, name = i['user']['id'], i['user']['screen_name']\n",
    "            name_list.append(name)\n",
    "            # print(id, name)\n",
    "\n",
    "        times = 2\n",
    "\n",
    "        while True:\n",
    "            url = f'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_{uid}&page={times}'\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "            if r.status_code == 200:\n",
    "                    cards = r.json()\n",
    "                    if cards.get('data') is not None:\n",
    "                        times += 1\n",
    "                        fan_list = r.json()['data']['cards'][-1]['card_group']\n",
    "                        for i in fan_list:\n",
    "                            id, name = i['user']['id'], i['user']['screen_name']\n",
    "                            # print(id, name)\n",
    "                            name_list.append(name)\n",
    "                    else:\n",
    "                        break\n",
    "    except:\n",
    "        return name_list\n",
    "\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def write_lab1(): # 记录账户微博基本信息并导出到表格\n",
    "    df = pd.DataFrame(columns = [\"name\", \"follow\", \"fans\", \"posts\"])\n",
    "\n",
    "    for i in uni_list:\n",
    "        value = getUserId(i)[1: 5]\n",
    "        print(value)\n",
    "        df.loc[len(df.index)] = value\n",
    "\n",
    "    df.to_csv(\"lab1.csv\", index = False)\n",
    "\n",
    "def search_follow():\n",
    "    for i in uni_list:\n",
    "        try:\n",
    "            print(i, getUserId(i))\n",
    "            scoll_search(getUserId(i)[0])\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        except:\n",
    "            print(\"-----\")\n",
    "            continue\n",
    "\n",
    "dict = {}\n",
    "def find_follow_as_friend():\n",
    "    \n",
    "    for i in uni_list:\n",
    "        dict[i] = scoll_search(getUserId(i)[0])\n",
    "\n",
    "    # print(dict)\n",
    "    result = np.zeros((len(uni_list), len(uni_list)))\n",
    "    temp = []\n",
    "    for key, value in dict.items():\n",
    "        temp.append(value)\n",
    "    # print(temp)\n",
    "\n",
    "    for count, value in enumerate(temp):\n",
    "        for uni in uni_list: \n",
    "            if uni in value:\n",
    "                result[count][uni_list.index(uni)] = 1\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df.columns = uni_list\n",
    "    df.index = uni_list\n",
    "    df.to_csv(\"lab3.csv\", index = False)\n",
    "\n",
    "    df\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uni_list = ['北京大学','清华大学','北京师范大学','北京外国语大学','北京交通大学','北京邮电大学','中国政法大学','中央财经大学','中央美术学院','对外经济贸易大学','南开大学','天津大学','大连理工大学','东北大学','吉林大学','东北师范大学','复旦大学','上海交通大学','同济大学','华东理工大学','东华大学','华东师范大学','上海外国语大学','南京大学','东南大学','河海大学','江南大学','南京农业大学','浙江大学','厦门大学','山东大学','中国海洋大学','中国石油大学（华东)','武汉大学','华中科技大学','中国地质大学（武汉)','武汉理工大学','华中农业大学','中南财经政法大学','湖南大学','中南大学','中山大学','重庆大学','西南大学','西南交通大学','西安交通大学','西北农林科技大学','陕西师范大学','西安电子科技大学','兰州大学','哈尔滨工业大学','哈尔滨工程大学','南京航空航天大学','中国科学技术大学','北京电影学院','辽宁大学','上海大学','南京林业大学','浙江工商大学','安徽大学','河南大学','武汉纺织大学','广西大学','西南政法大学','四川师范大学','贵州大学']\n",
    "    find_follow_as_friend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# result = np.zeros((len(uni_list), len(uni_list)))\n",
    "\n",
    "# temp = []\n",
    "# for key, value in dict.items():\n",
    "#     temp.append(value)\n",
    "# # print(temp)\n",
    "\n",
    "# for count, value in enumerate(temp):\n",
    "#     for uni in uni_list: \n",
    "#         if uni in value:\n",
    "#             result[count][uni_list.index(uni)] = 1\n",
    "\n",
    "# df = pd.DataFrame(result)\n",
    "# df.columns = uni_list\n",
    "# df.index = uni_list\n",
    "# df.to_csv(\"lab3.csv\", index = False)\n",
    "\n",
    "# df\n",
    "\n",
    "# 导出微博关注用户\n",
    "# df = pd.DataFrame(columns = [\"name\", \"follow\"])\n",
    "# for i, j in dict.items():\n",
    "#     print(i, [itm for itm in j])\n",
    "\n",
    "#     df.loc[len(df.index)] = i, j\n",
    "\n",
    "# # df.to_csv(\"lab2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "uid = 3237705130\n",
    "\n",
    "headers = {\n",
    "'Cookie': '_T_WM=69514023915; MLOGIN=1; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfq6k41yT5HWkNhZ5TIvDrF5_t-RjdCZ-1J3mknscKAxjU.; SUB=_2A25O3rybDeRhGeBP71YU8y7KzTuIHXVqIMTTrDV6PUJbktANLWfxkW1NRUYLcUYi8i0Y-SvhDTT3Qd54PBXW9h3c; SSOLoginState=1675283659; ALF=1677875659; WEIBOCN_FROM=1110106030; XSRF-TOKEN=1293ca; mweibo_short_token=60cc77794f; M_WEIBOCN_PARAMS=luicode%3D10000011%26lfid%3D231051_-_followers_-_1764661771%26fid%3D231051_-_followers_-_1764661771%26uicode%3D10000011',\n",
    "'Referer': f'https://m.weibo.cn/p/index?containerid=231051_-_followers_-_{uid}',\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "'X-Requested-With': 'XMLHttpRequest'\n",
    "}\n",
    "\n",
    "url = f'https://m.weibo.cn/api/container/getIndex?containerid=231051_-_followers_-_{uid}'\n",
    "print(url)\n",
    "print(headers)\n",
    "r = requests.get(url, headers = headers, timeout = 5)\n",
    "print(r.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 草稿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = \"[\\u4e00-\\u9fa5]{5}\"\n",
    "\n",
    "# matches = re.findall(pattern, string)\n",
    "\n",
    "# if matches:\n",
    "#     for match in matches:\n",
    "#         print(match, end = \"\")\n",
    "# else:\n",
    "#     print(\"匹配失败\")\n",
    "\n",
    "# except:\n",
    "# \t\tcontinue\n",
    "\n",
    "# finally:\n",
    "# \ttime.sleep(random.uniform(0, 2))\n",
    "# \ttimes += 1\n",
    "\n",
    "# # 转发数\n",
    "# reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "# # 评论数\n",
    "# comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "# # 点赞数\n",
    "# attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "\n",
    "# # 请求地址\n",
    "# url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "# # 请求参数\n",
    "# params = {\n",
    "# \t\"containerid\": \"100103type=1&q={}\".format(topic),\n",
    "# \t\"page_type\": \"searchall\",\n",
    "# \t# \"page\": searchall\n",
    "# }\n",
    "# r = requests.get(url, headers = headers, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        return requests.get(url).text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'Crawling: {url}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://www.imdb.com/']).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
