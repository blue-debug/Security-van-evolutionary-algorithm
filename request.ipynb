{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://m.weibo.cn/statuses/extend?id=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     25\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, timeout \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m string \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mjson()[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlongTextContent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[39m# print(string)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[39m# export to csv\u001b[39;00m\n\u001b[1;32m     31\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py:897\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m             \u001b[39m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             \u001b[39m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[1;32m    894\u001b[0m             \u001b[39m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[1;32m    895\u001b[0m             \u001b[39m# used.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39;49mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/simplejson/__init__.py:518\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39m\"\"\"Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39mdocument) to a Python object.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m \n\u001b[1;32m    513\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    515\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    516\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m use_decimal \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 518\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py:370\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m _PY3 \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(s, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    369\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(s, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding)\n\u001b[0;32m--> 370\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s)\n\u001b[1;32m    371\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py:400\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39melif\u001b[39;00m ord0 \u001b[39m==\u001b[39m \u001b[39m0xef\u001b[39m \u001b[39mand\u001b[39;00m s[idx:idx \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\xef\u001b[39;00m\u001b[39m\\xbb\u001b[39;00m\u001b[39m\\xbf\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[1;32m    399\u001b[0m         idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx\u001b[39m=\u001b[39;49m_w(s, idx)\u001b[39m.\u001b[39;49mend())\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "def trans_time(v_str):\n",
    "\t\"\"\"转换GMT时间为标准格式\"\"\"\n",
    "\tGMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "\ttimeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "\tret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\treturn ret_time\n",
    "\n",
    "\n",
    "id = 4860690891018271\n",
    "mid, max_id = id, ''\n",
    "cookie = 'WEIBOCN_FROM=1110006030; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfqPajGN6tNkHrnFHqQBP4OlYtShy_Or1r2ELmJDriqh-I.; SUB=_2A25OzHZUDeRhGeBP71YU8y7KzTuIHXVqTxocrDV6PUJbktAKLVfBkW1NRUYLcaFm2Y6WDEzl3u_svMh_CqKt1m1s; SSOLoginState=1674053124; ALF=1676645124; _T_WM=54103595383; MLOGIN=1; XSRF-TOKEN=f69380; mweibo_short_token=b0e76dc5fb; M_WEIBOCN_PARAMS=oid%3D4860690891018271%26luicode%3D20000061%26lfid%3D4860690891018271%26uicode%3D20000061%26fid%3D4860690891018271'\n",
    "\n",
    "url = f'https://m.weibo.cn/statuses/extend?id={id}'\n",
    "r = requests.get(url, timeout = 5)\n",
    "\n",
    "string = r.json()['data']['longTextContent']\n",
    "# print(string)\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame(columns = [\"index\", \"text\"])\n",
    "df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n",
    "df.to_csv(\"spider.csv\", index = False)\n",
    "\n",
    "# pattern = \"[\\u4e00-\\u9fa5]{5}\"\n",
    "\n",
    "# matches = re.findall(pattern, string)\n",
    "\n",
    "# if matches:\n",
    "#     for match in matches:\n",
    "#         print(match, end = \"\")\n",
    "# else:\n",
    "#     print(\"匹配失败\")\n",
    "\n",
    "\n",
    "def sub_comment(comment_id):\n",
    "    headers = {\n",
    "    f'Cookie': '{cookie}',\n",
    "    f'Referer': 'https://m.weibo.cn/detail/{id}?cid={comment_id}',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "\n",
    "    max_id = 0\n",
    "    id_type = 0\n",
    "    max = 1\n",
    "    times = 0\n",
    "\n",
    "    temp_index, temp_text = [], []\n",
    "\n",
    "    while times < max:\n",
    "\n",
    "        url = f'https://m.weibo.cn/comments/hotFlowChild?cid={comment_id}&max_id={max_id}&max_id_type={id_type}'\n",
    "        print(url)\n",
    "        r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "        try:\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()['data']\n",
    "                \n",
    "                max_id = r.json()['max_id']\n",
    "                id_type = r.json()['max_id_type']\n",
    "                max = r.json()['max']\n",
    "\n",
    "                for i in cards:\n",
    "                    print(i['text'])\n",
    "                    temp_index.append(i[\"id\"], ignore_index = True)\n",
    "                    temp_text.append(i[\"text\"], ignore_index = True)\n",
    "\n",
    "                time.sleep(1)\n",
    "                times += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"max not found in sub comment\")\n",
    "            print(e)\n",
    "            return temp_index, temp_text\n",
    "    \n",
    "    return temp_index, temp_text\n",
    "\n",
    "# 第一条评论\n",
    "headers = {\n",
    "    f'Cookie': '{cookie}',\n",
    "    'Referer': 'https://m.weibo.cn/detail/4860690891018271',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "    'X-Requested-With': 'XMLHttpRequest'\n",
    "}\n",
    "\n",
    "max = 1\n",
    "times = 0\n",
    "\n",
    "# 滚动评论\n",
    "url = f'https://m.weibo.cn/comments/hotflow?id={id}&mid={mid}&max_id={max_id}&max_id_type={id_type}' if max_id else f'https://m.weibo.cn/comments/hotflow?id={id}&mid={mid}&max_id_type=0'\n",
    "print(url)\n",
    "\n",
    "while times < max:\n",
    "    r = requests.get(url, headers = headers, timeout = 5)\n",
    "    cards = r.json()['data']['data']\n",
    "    temp_index, temp_text = [], []\n",
    "\n",
    "    try:\n",
    "        for i in cards:\n",
    "            print(i['id'], i['text'])\n",
    "            temp_index.append(i['id'])\n",
    "            temp_text.append(i['text'])\n",
    "\n",
    "            a, b = sub_comment(i['id'])\n",
    "            if a and b:\n",
    "                temp_index.append(a)\n",
    "                temp_text.append(b)\n",
    "\n",
    "        df = pd.DataFrame({\"index\": temp_index, \"text\": temp_text})\n",
    "        # df = df.append({\"index\": cards['id'], \"text\": cards['text']}, ignore_index = True)\n",
    "        \n",
    "        df.to_csv(\"spider.csv\", index = False, mode = 'a', header = False)\n",
    "\n",
    "        max_id = r.json()['data']['max_id']\n",
    "        id_type = r.json()['data']['max_id_type']\n",
    "    \n",
    "        max = r.json()['max']\n",
    "    except Exception as e:\n",
    "        print(\"max not found\")\n",
    "        print(e)\n",
    "        continue\n",
    "            \n",
    "    time.sleep(random.uniform(0, 2))\n",
    "\n",
    "    times += 1\n",
    "\n",
    "\t# except:\n",
    "\t# \t\tcontinue\n",
    "\t\n",
    "\t# finally:\n",
    "\t# \ttime.sleep(random.uniform(0, 2))\n",
    "\t# \ttimes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 转发数\n",
    "# reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "# # 评论数\n",
    "# comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "# # 点赞数\n",
    "# attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "\n",
    "# # 请求地址\n",
    "# url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "# # 请求参数\n",
    "# params = {\n",
    "# \t\"containerid\": \"100103type=1&q={}\".format(topic),\n",
    "# \t\"page_type\": \"searchall\",\n",
    "# \t# \"page\": searchall\n",
    "# }\n",
    "# r = requests.get(url, headers = headers, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        return requests.get(url).text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'Crawling: {url}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://www.imdb.com/']).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
