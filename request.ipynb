{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, id = 4860690891018271):\n",
    "        self.id = id\n",
    "        self.mid = id\n",
    "        \n",
    "        self.cookie = 'WEIBOCN_FROM=1110006030; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfqPajGN6tNkHrnFHqQBP4OlYtShy_Or1r2ELmJDriqh-I.; SUB=_2A25OzHZUDeRhGeBP71YU8y7KzTuIHXVqTxocrDV6PUJbktAKLVfBkW1NRUYLcaFm2Y6WDEzl3u_svMh_CqKt1m1s; SSOLoginState=1674053124; ALF=1676645124; MLOGIN=1; _T_WM=82331204434; XSRF-TOKEN=096b86; mweibo_short_token=d6391958c3; M_WEIBOCN_PARAMS=oid%3D4860690891018271%26luicode%3D20000061%26lfid%3D4860690891018271%26uicode%3D20000061%26fid%3D4860690891018271'\n",
    "\n",
    "        self.url = f'https://m.weibo.cn/statuses/extend?id={self.id}'\n",
    "\n",
    "        r = requests.get(self.url, timeout = 5)\n",
    "        string = r.json()['data']['longTextContent']\n",
    "        # df = pd.DataFrame(columns = [\"index\", \"text\"])\n",
    "        # df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n",
    "        df = pd.DataFrame({\"name\": [''], \"粉丝数\": [''], \"text\": [string], \"like_count\": ['']})\n",
    "        df.to_csv(\"spider.csv\", index = False)\n",
    "    \n",
    "\n",
    "    def sub_comment(self, comment_id):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id + '?cid=' + comment_id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        print(headers)\n",
    "\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "        var_index, var_text = [], []\n",
    "        var_like_count = []\n",
    "        var_name = []\n",
    "        var_fans = []\n",
    "        var_source = []\n",
    "\n",
    "        while times < max:\n",
    "\n",
    "            url = f'https://m.weibo.cn/comments/hotFlowChild?cid={comment_id}&max_id={max_id}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = cards['data']\n",
    "\n",
    "                    for i in cards:\n",
    "                        # print(i['text'])\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "                    \n",
    "                    max_id = r.json()['max_id']\n",
    "                    id_type = r.json()['max_id_type']\n",
    "                    max = r.json()['max']\n",
    "\n",
    "                    # time.sleep(1)\n",
    "                    times += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"return data is null\")\n",
    "                    break\n",
    "        \n",
    "        return [var_name, var_fans, var_text, var_like_count]\n",
    "    \n",
    "\n",
    "    def comment(self):\n",
    "        headers = {\n",
    "            'Cookie': self.cookie,\n",
    "            'Referer': 'https://m.weibo.cn/detail/' + self.id,\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        print(\"main\")\n",
    "        print(headers)\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "\n",
    "        while times < max:\n",
    "            print(f'当前次数{times}为， 总共{max}次')\n",
    "            url = f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id={max_id}&max_id_type={id_type}' if max_id >= 1 else f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            \n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                cards = r.json()\n",
    "                if cards.get('data') is not None:\n",
    "                    cards = r.json()['data']['data']\n",
    "                    var_text = []\n",
    "                    var_like_count = []\n",
    "                    var_name = []\n",
    "                    var_fans = []\n",
    "                    var_source = []\n",
    "\n",
    "                    time.sleep(random.uniform(0, 2))\n",
    "                    times += 1 \n",
    "\n",
    "                    # try:\n",
    "                    for i in cards:\n",
    "                        # print(i['id'], i['text'])\n",
    "\n",
    "                        var_name.append(i['user']['screen_name'])\n",
    "                        var_fans.append(i['user']['followers_count'])\n",
    "                        var_text.append(i['text'])\n",
    "                        var_like_count.append(i['like_count'])\n",
    "\n",
    "                        return_var = self.sub_comment(i['id'])\n",
    "\n",
    "                        var_name += return_var[0]\n",
    "                        var_fans.extend(return_var[1])\n",
    "                        var_text += return_var[2]\n",
    "                        var_like_count.extend(return_var[3])\n",
    "\n",
    "                    df = pd.DataFrame({\"name\": var_name, \"粉丝数\": var_fans, \"text\": var_text, \"like_count\": var_like_count})\n",
    "                    # df = df.append({\"index\": cards['id'], \"text\": cards['text']}, ignore_index = True)\n",
    "                    \n",
    "                    df.to_csv(\"spider.csv\", index = False, mode = 'a', header = False)\n",
    "\n",
    "                    if times == 1: max = r.json()['data']['max']\n",
    "\n",
    "                    max_id = r.json()['data']['max_id']\n",
    "                    id_type = r.json()['data']['max_id_type']\n",
    "                \n",
    "                else:\n",
    "                    print(\"data is null\")\n",
    "                    continue\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(\"max not found\")\n",
    "            #     print(e)\n",
    "            #     continue\n",
    "    \n",
    "    def trans_time(self, v_str):\n",
    "        \"\"\"转换GMT时间为标准格式\"\"\"\n",
    "        GMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "        timeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "        ret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return ret_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        spider = Crawler('4860690891018271').comment()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user. Exiting...\")\n",
    "        del spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = \"[\\u4e00-\\u9fa5]{5}\"\n",
    "\n",
    "# matches = re.findall(pattern, string)\n",
    "\n",
    "# if matches:\n",
    "#     for match in matches:\n",
    "#         print(match, end = \"\")\n",
    "# else:\n",
    "#     print(\"匹配失败\")\n",
    "\n",
    "# except:\n",
    "# \t\tcontinue\n",
    "\n",
    "# finally:\n",
    "# \ttime.sleep(random.uniform(0, 2))\n",
    "# \ttimes += 1\n",
    "\n",
    "# # 转发数\n",
    "# reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "# # 评论数\n",
    "# comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "# # 点赞数\n",
    "# attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "\n",
    "# # 请求地址\n",
    "# url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "# # 请求参数\n",
    "# params = {\n",
    "# \t\"containerid\": \"100103type=1&q={}\".format(topic),\n",
    "# \t\"page_type\": \"searchall\",\n",
    "# \t# \"page\": searchall\n",
    "# }\n",
    "# r = requests.get(url, headers = headers, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        return requests.get(url).text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'Crawling: {url}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://www.imdb.com/']).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
