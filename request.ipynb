{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  # 正则表达式提取文本\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import requests  # 发送请求\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, id = 4860690891018271):\n",
    "        self.id = id\n",
    "        self.mid = id\n",
    "        \n",
    "        self.cookie = 'WEIBOCN_FROM=1110006030; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFfYb7mkA6BHfpdMmuxGQ8k5JpX5K-hUgL.FoqpShBfe05cSoM2dJLoIE-LxKqLBoMLBo2LxKnL1KeL1-BLxKBLBo.L12zLxK-LB.qL1hSo; SCF=At3cKMF8zNNlQvzopN0jE4GXnW6d1oMHWhTaIC30tCfqPajGN6tNkHrnFHqQBP4OlYtShy_Or1r2ELmJDriqh-I.; SUB=_2A25OzHZUDeRhGeBP71YU8y7KzTuIHXVqTxocrDV6PUJbktAKLVfBkW1NRUYLcaFm2Y6WDEzl3u_svMh_CqKt1m1s; SSOLoginState=1674053124; ALF=1676645124; _T_WM=54103595383; MLOGIN=1; XSRF-TOKEN=f69380; mweibo_short_token=b0e76dc5fb; M_WEIBOCN_PARAMS=oid%3D4860690891018271%26luicode%3D20000061%26lfid%3D4860690891018271%26uicode%3D20000061%26fid%3D4860690891018271'\n",
    "\n",
    "        self.url = f'https://m.weibo.cn/statuses/extend?id={self.id}'\n",
    "\n",
    "        r = requests.get(self.url, timeout = 5)\n",
    "        string = r.json()['data']['longTextContent']\n",
    "        df = pd.DataFrame(columns = [\"index\", \"text\"])\n",
    "        df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n",
    "        df.to_csv(\"spider.csv\", index = False)\n",
    "\n",
    "    def sub_comment(self, comment_id):\n",
    "        headers = {\n",
    "        f'Cookie': '{self.cookie}',\n",
    "        f'Referer': 'https://m.weibo.cn/detail/{self.id}?cid={comment_id}',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "        'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "        temp_index, temp_text = [], []\n",
    "\n",
    "        while times < max:\n",
    "\n",
    "            url = f'https://m.weibo.cn/comments/hotFlowChild?cid={comment_id}&max_id={max_id}&max_id_type={id_type}'\n",
    "            print(url)\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "\n",
    "            try:\n",
    "                cards = r.json()['data']\n",
    "                \n",
    "                max_id = r.json()['max_id']\n",
    "                id_type = r.json()['max_id_type']\n",
    "                if times == 0: max = r.json()['max']\n",
    "\n",
    "                for i in cards:\n",
    "                    print(i['text'])\n",
    "                    temp_index.append(i[\"id\"])\n",
    "                    temp_text.append(i[\"text\"])\n",
    "\n",
    "                time.sleep(random.uniform(0, 1))\n",
    "                times += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"max not found in sub comment\")\n",
    "                print(e)\n",
    "                return temp_index, temp_text\n",
    "        \n",
    "        return temp_index, temp_text\n",
    "\n",
    "    def comment(self):\n",
    "        # 第一条评论\n",
    "        headers = {\n",
    "            f'Cookie': '{cookie}',\n",
    "            'Referer': 'https://m.weibo.cn/detail/4860690891018271',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.61',\n",
    "            'X-Requested-With': 'XMLHttpRequest'\n",
    "        }\n",
    "\n",
    "        times, max = 0, 1\n",
    "        max_id, id_type = 0, 0\n",
    "\n",
    "        # 滚动评论\n",
    "        url = f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id={max_id}&max_id_type={id_type}' if max_id else f'https://m.weibo.cn/comments/hotflow?id={self.id}&mid={self.mid}&max_id_type=0'\n",
    "        print(url)\n",
    "\n",
    "        while times < max:\n",
    "            r = requests.get(url, headers = headers, timeout = 5)\n",
    "            cards = r.json()['data']['data']\n",
    "            temp_index, temp_text = [], []\n",
    "\n",
    "            time.sleep(random.uniform(0, 2))\n",
    "            times += 1 \n",
    "\n",
    "            try:\n",
    "                for i in cards:\n",
    "                    print(i['id'], i['text'])\n",
    "                    temp_index.append(i['id'])\n",
    "                    temp_text.append(i['text'])\n",
    "\n",
    "                    a, b = sub_comment(i['id'])\n",
    "                    temp_index += a\n",
    "                    temp_text.extend(b)\n",
    "\n",
    "                    temp_index += ' '\n",
    "                    temp_text += ' '\n",
    "\n",
    "                df = pd.DataFrame({\"index\": temp_index, \"text\": temp_text})\n",
    "                # df = df.append({\"index\": cards['id'], \"text\": cards['text']}, ignore_index = True)\n",
    "                \n",
    "                df.to_csv(\"spider.csv\", index = False, mode = 'a', header = False)\n",
    "\n",
    "                if times == 0: max = r.json()['max']\n",
    "\n",
    "                max_id = r.json()['data']['max_id']\n",
    "                id_type = r.json()['data']['max_id_type']\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"max not found\")\n",
    "                print(e)\n",
    "                continue\n",
    "    \n",
    "    def trans_time(v_str):\n",
    "        \"\"\"转换GMT时间为标准格式\"\"\"\n",
    "        GMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "        timeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "        ret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return ret_time\n",
    "\n",
    "    # pattern = \"[\\u4e00-\\u9fa5]{5}\"\n",
    "\n",
    "    # matches = re.findall(pattern, string)\n",
    "\n",
    "    # if matches:\n",
    "    #     for match in matches:\n",
    "    #         print(match, end = \"\")\n",
    "    # else:\n",
    "    #     print(\"匹配失败\")\n",
    "\n",
    "\t# except:\n",
    "\t# \t\tcontinue\n",
    "\t\n",
    "\t# finally:\n",
    "\t# \ttime.sleep(random.uniform(0, 2))\n",
    "\t# \ttimes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2210310/3412914135.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({\"index\": id, \"text\": string}, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Crawler(4860690891018271).comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 转发数\n",
    "# reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "# # 评论数\n",
    "# comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "# # 点赞数\n",
    "# attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "\n",
    "# # 请求地址\n",
    "# url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "# # 请求参数\n",
    "# params = {\n",
    "# \t\"containerid\": \"100103type=1&q={}\".format(topic),\n",
    "# \t\"page_type\": \"searchall\",\n",
    "# \t# \"page\": searchall\n",
    "# }\n",
    "# r = requests.get(url, headers = headers, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self, urls=[]):\n",
    "        self.visited_urls = []\n",
    "        self.urls_to_visit = urls\n",
    "\n",
    "    def download_url(self, url):\n",
    "        return requests.get(url).text\n",
    "\n",
    "    def get_linked_urls(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            path = link.get('href')\n",
    "            if path and path.startswith('/'):\n",
    "                path = urljoin(url, path)\n",
    "            yield path\n",
    "\n",
    "    def add_url_to_visit(self, url):\n",
    "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
    "            self.urls_to_visit.append(url)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        html = self.download_url(url)\n",
    "        for url in self.get_linked_urls(url, html):\n",
    "            self.add_url_to_visit(url)\n",
    "\n",
    "    def run(self):\n",
    "        while self.urls_to_visit:\n",
    "            url = self.urls_to_visit.pop(0)\n",
    "            logging.info(f'Crawling: {url}')\n",
    "            try:\n",
    "                self.crawl(url)\n",
    "            except Exception:\n",
    "                logging.exception(f'Failed to crawl: {url}')\n",
    "            finally:\n",
    "                self.visited_urls.append(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Crawler(urls=['https://www.imdb.com/']).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
